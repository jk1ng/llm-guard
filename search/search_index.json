{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Guard - The Security Toolkit for LLM Interactions","text":"<p>LLM Guard by Laiyer.ai is a comprehensive tool designed to fortify the security of Large Language Models (LLMs).</p>"},{"location":"#what-is-llm-guard","title":"What is LLM Guard?","text":"<p>By offering sanitization, detection of harmful language, prevention of data leakage, and resistance against prompt injection attacks, LLM-Guard ensures that your interactions with LLMs remain safe and secure.</p> <p>Demo</p>"},{"location":"#installation","title":"Installation","text":"<p>Begin your journey with LLM Guard by downloading the package:</p> <pre><code>pip install llm-guard\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Important Notes:</p> <ul> <li>LLM Guard is designed for easy integration and deployment in production environments. While it's ready to use   out-of-the-box, please be informed that we're constantly improving and updating the repository.</li> <li>Base functionality requires a limited number of libraries. As you explore more advanced features, necessary libraries   will be automatically installed.</li> <li>Ensure you're using Python version 3.8.1 or higher. Confirm with: <code>python --version</code>.</li> <li>Library installation issues? Consider upgrading pip: <code>python -m pip install --upgrade pip</code>.</li> </ul> <p>Examples:</p> <ul> <li>Get started with ChatGPT and LLM Guard.</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":""},{"location":"#general","title":"General","text":"<ul> <li> Extend language support to cover popular and emerging languages, prioritize based on community feedback.</li> <li> Allow comparison of multiple outputs to facilitate better analysis and choice.</li> <li> Enable scanning of logits to support streaming mode.</li> <li> Expand examples and integrations, ensuring they cover common use-cases and are easy to follow.</li> </ul>"},{"location":"#latency","title":"Latency","text":"<ul> <li> Implement parallel scanning using multiprocessing to significantly reduce scanning time.</li> <li> Provide an option to utilize lighter models for quicker scanning, while maintaining an acceptable level of accuracy.</li> <li> Incorporate LRU cache to optimize performance by reusing previous results where applicable.</li> </ul>"},{"location":"#prompt-scanners","title":"Prompt Scanners","text":"<ul> <li> Allow language restriction to focus scanning efforts and improve accuracy.</li> <li> Utilize expressions for code detection to reduce dependency on models, improving speed and reliability.</li> <li> Integrate yara for secret detection to enhance security scanning capabilities.</li> <li> Sanitize text.</li> <li> Support a variety of token calculators to offer more flexibility and compatibility.</li> </ul>"},{"location":"#output-scanners","title":"Output Scanners","text":"<ul> <li> Sanitize text to maintain a clean, accurate scanning process.</li> <li> Validate output formats like JSON, XML to ensure they adhere to standards.</li> <li> Incorporate factual consistency checking to uphold the reliability of the data.</li> <li> Scan for vulnerable libraries and provide recommendations for safer alternatives.</li> <li> Check for license compliance to ensure legal integrity.</li> <li> Detect insecure code patterns.</li> <li> Identify potential SQL injection points to enhance security.</li> <li> Verify links and provide options for whitelisting or blacklisting to maintain the quality of references.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Got ideas, feedback, or wish to contribute? We'd love to hear from you! Email us.</p> <p>For detailed guidelines on contributions, kindly refer to our contribution guide.</p>"},{"location":"best_practices/","title":"Best Practices","text":""},{"location":"best_practices/#performance-optimization","title":"Performance Optimization","text":"<ol> <li> <p>Benchmark Analysis: Before choosing the scanners, it's crucial to understand their performance on different instances. Review the benchmarks for each scanner to make an informed decision based on your specific requirements.</p> </li> <li> <p>Model Size Trade-off: Opting for smaller models will expedite processing, reducing latency. However, this comes at the cost of accuracy. We are actively working on providing compact versions with minimal accuracy trade-offs.</p> </li> </ol>"},{"location":"best_practices/#serving-configurations","title":"Serving Configurations","text":"<ol> <li> <p>Fast Failure Mode: Enable the <code>fail_fast</code> mode while serving to ensure early exits, preventing the wait for all scanners to complete, thus optimizing the response time.</p> </li> <li> <p>Scanner Selection: Assess the relevance of different scanners for your use-case. Instead of employing all scanners synchronously, which might overwhelm the system, consider using them asynchronously. This approach enhances observability, aiding in precise debugging and performance monitoring.</p> </li> </ol>"},{"location":"best_practices/#observability-and-debugging","title":"Observability and Debugging","text":"<ol> <li>Logging and Metrics: Implement robust logging and metric collection to monitor the system's performance and health.</li> </ol>"},{"location":"best_practices/#continuous-improvement","title":"Continuous Improvement","text":"<ol> <li> <p>Feedback Loops: Establish feedback loops with your system's users to understand how the library is performing in real-world scenarios, and to gather suggestions for improvements.</p> </li> <li> <p>Regular Updates and Testing: Stay updated with the latest versions of <code>llm-guard</code>, and ensure thorough testing in a staging environment before rolling out updates in a production setup.</p> </li> </ol>"},{"location":"installation/","title":"Installing LLM Guard","text":"<p>This document describes how to download and install the LLM Guard locally.</p>"},{"location":"installation/#supported-python-versions","title":"Supported Python Versions","text":"<p>LLM Guard is supported for the following python versions:</p> <ul> <li>3.9</li> <li>3.10</li> <li>3.11</li> </ul>"},{"location":"installation/#using-pip","title":"Using <code>pip</code>","text":"<p>Note</p> <p>Consider installing the LLM Guard python packages on a virtual environment like <code>venv</code> or <code>conda</code>.</p> <pre><code>pip install llm-guard\n</code></pre>"},{"location":"installation/#install-from-source","title":"Install from source","text":"<p>To install LLM Guard from source, first clone the repo:</p> <ul> <li>Using HTTPS <pre><code>git clone https://github.com/laiyer-ai/llm-guard.git\n</code></pre></li> <li>Using SSH <pre><code>git clone git@github.com:laiyer-ai/llm-guard.git\n</code></pre></li> </ul> <p>Then, install the package using <code>pip</code>:</p> <pre><code># install the repo\npip install -U -r requirements.txt -r requirements-dev.txt\npython setup.py install\n</code></pre>"},{"location":"quickstart/","title":"Getting started with LLM Guard","text":"<p>Each scanner can be used individually, or using the <code>scan_prompt</code> function.</p>"},{"location":"quickstart/#individual","title":"Individual","text":"<p>You can import an individual scanner and use it to evaluate the prompt or the output:</p> <pre><code>from llm_guard.input_scanners import BanTopics\n\nscanner = BanTopics(topics=[\"violence\"], threshold=0.5)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <pre><code>from llm_guard.output_scanners import Bias\n\nscanner = Bias(threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"quickstart/#multiple","title":"Multiple","text":"<p>Info</p> <p>Scanners are executed in the order they are passed to the <code>scan_prompt</code> function.</p> <p>For prompt:</p> <pre><code>from llm_guard import scan_prompt\nfrom llm_guard.input_scanners import Anonymize, PromptInjection, TokenLimit, Toxicity\nfrom llm_guard.vault import Vault\n\nvault = Vault()\ninput_scanners = [Anonymize(vault), Toxicity(), TokenLimit(), PromptInjection()]\n\nsanitized_prompt, results_valid, results_score = scan_prompt(input_scanners, prompt)\nif any(not result for result in results_valid.values()):\n    print(f\"Prompt {prompt} is not valid, scores: {results_score}\")\n    exit(1)\n\nprint(f\"Prompt: {sanitized_prompt}\")\n</code></pre> <p>For output:</p> <pre><code>from llm_guard import scan_output\nfrom llm_guard.output_scanners import Deanonymize, NoRefusal, Relevance, Sensitive\n\nvault = Vault()\noutput_scanners = [Deanonymize(vault), NoRefusal(), Relevance(), Sensitive()]\n\nsanitized_response_text, results_valid, results_score = scan_output(\n    output_scanners, sanitized_prompt, response_text\n)\nif any(not result for result in results_valid.values()):\n    print(f\"Output {response_text} is not valid, scores: {results_score}\")\n    exit(1)\n\nprint(f\"Output: {sanitized_response_text}\\n\")\n</code></pre> <p>Note</p> <p>You can set <code>fail_fast</code> to <code>True</code> to stop scanning after the first invalid result. This can help to reduce the latency of the scanning.</p>"},{"location":"customization/add_scanner/","title":"Adding a new scanner","text":"<p>LLM Guard can be extended to support new scanners, and to support additional models for the existing. These scanners could be added via code or ad-hoc as part of the request.</p> <p>Note</p> <p>Before writing code, please read the contributing guide.</p>"},{"location":"customization/add_scanner/#extending-the-input-prompt-scanners","title":"Extending the input (prompt) scanners","text":"<ol> <li>Create a new class in the <code>llm_guard/input_scanners</code> that inherits from <code>base.Scanner</code> and implements the <code>scan</code> method. The <code>scan</code> method should return a tuple <code>str, bool, float</code>.</li> <li>Add test cases for the new scanner in <code>tests/input_scanners</code>.</li> <li>Add the new scanner to the <code>llm_guard/input_scanners/__init__.py</code> <code>__all__</code> enum.</li> <li>Write documentation in the <code>docs/input_scanners</code> folder and add a link to the <code>mkdocs.yml</code> file.</li> <li>Also, add a link to the documentation in <code>README.md</code>, and update the <code>CHANGELOG.md</code> file.</li> </ol>"},{"location":"customization/add_scanner/#extending-the-output-scanners","title":"Extending the output scanners","text":"<ol> <li>Create a new class in the <code>llm_guard/output_scanners</code> that inherits from <code>base.Scanner</code> and implements the <code>scan</code> method. The <code>scan</code> method should return a tuple <code>str, bool, float</code>.</li> <li>Add test cases for the new scanner in <code>tests/output_scanners</code>.</li> <li>Add the new scanner to the <code>llm_guard/output_scanners/__init__.py</code> <code>__all__</code> enum.</li> <li>Write documentation in the <code>docs/output_scanners</code> folder and add a link to the <code>mkdocs.yml</code> file.</li> <li>Also, add a link to the documentation in <code>README.md</code>, and update the <code>CHANGELOG.md</code> file.</li> </ol> <p>Info</p> <p>You can use existing scanners as a reference.</p>"},{"location":"input_scanners/anonymize/","title":"Anonymize Scanner","text":"<p>The <code>Anonymize</code> Scanner acts as your digital guardian, ensuring your user prompts remain confidential and free from sensitive data exposure.</p>"},{"location":"input_scanners/anonymize/#what-is-pii","title":"What is PII?","text":"<p>PII, an acronym for Personally Identifiable Information, is the cornerstone of an individual's digital identity. Leaks or mishandling of PII can unleash a storm of problems, from privacy breaches to identity theft. Global regulations, including GDPR and HIPAA, underscore the significance of PII by laying out strict measures for its protection. Furthermore, any unintentional dispatch of PII to LLMs can proliferate this data across various storage points, thus raising the stakes.</p>"},{"location":"input_scanners/anonymize/#attack","title":"Attack","text":"<p>Sometimes, Language Learning Models (or LLMs) can accidentally share private info from the prompts they get. This can be bad because it might let others see or use this info in the wrong way.</p> <p>To stop this from happening, we use the <code>Anonymize</code> scanner. It makes sure user prompts don\u2019t have any private details before the model sees them.</p>"},{"location":"input_scanners/anonymize/#pii-entities","title":"PII Entities","text":"<ul> <li>Credit Cards: Formats mentioned in Wikipedia.</li> <li><code>4111111111111111</code></li> <li><code>378282246310005</code> (American Express)</li> <li><code>30569309025904</code> (Diners Club)</li> <li>Person: A full person name, which can include first names, middle names or initials, and last names.</li> <li><code>John Doe</code></li> <li>PHONE_NUMBER:</li> <li><code>5555551234</code></li> <li>URL: A URL (Uniform Resource Locator), unique identifier used to locate a resource on the Internet.</li> <li><code>https://laiyer.ai</code></li> <li>E-mail Addresses: Standard email formats.</li> <li><code>john.doe@laiyer.ai</code></li> <li><code>john.doe[AT]laiyer[DOT]ai</code></li> <li><code>john.doe[AT]laiyer.ai</code></li> <li><code>john.doe@laiyer[DOT]ai</code></li> <li>IPs: An Internet Protocol (IP) address (either IPv4 or IPv6).</li> <li><code>192.168.1.1</code> (IPv4)</li> <li><code>2001:db8:3333:4444:5555:6666:7777:8888</code> (IPv6)</li> <li>UUID:</li> <li><code>550e8400-e29b-41d4-a716-446655440000</code></li> <li>US Social Security Number (SSN):</li> <li><code>111-22-3333</code></li> <li>Crypto wallet number: Currently only Bitcoin address is supported.</li> <li><code>1Lbcfr7sAHTD9CgdQo3HTMTkV8LK4ZnX71</code></li> <li>IBAN Code: The International Bank Account Number (IBAN) is an internationally agreed system of identifying bank   accounts across national borders to facilitate the communication and processing of cross border transactions with a   reduced risk of transcription errors.</li> <li><code>DE89370400440532013000</code></li> </ul>"},{"location":"input_scanners/anonymize/#features","title":"Features","text":"<ul> <li>Integration with Presidio Analyzer: Leverages the Presidio Analyzer   library, crafted with spaCy, flair and transformers libraries, for precise detection of private data.</li> <li>Enhanced Detection: Beyond Presidio Analyzer's capabilities, the scanner recognizes specific patterns like Email,   US SSN, UUID, and more.</li> <li>Entities Support:</li> <li>Peek at     our default entities.</li> <li>View     the Presidio's supported entities.</li> <li>And, we've     got custom regex patterns     too!</li> <li>Tailored Recognizers:</li> <li>Balance speed vs. accuracy of the recognizers.</li> <li>Top Pick: dslim/bert-base-NER</li> <li>Alternatives: dslim/bert-large-NER.</li> </ul> <p>Info</p> <p>Current entity detection functionality is English-specific.</p>"},{"location":"input_scanners/anonymize/#get-started","title":"Get Started","text":"<p>Initialize the <code>Vault</code>: The Vault archives data that's been redacted.</p> <pre><code>from llm_guard.vault import Vault\n\nvault = Vault()\n</code></pre> <p>Configure the <code>Anonymize</code> Scanner:</p> <pre><code>from llm_guard.input_scanners import Anonymize\nfrom llm_guard.input_scanners.anonymize_helpers import BERT_LARGE_NER_CONF\n\nscanner = Anonymize(vault, preamble=\"Insert before prompt\", allowed_names=[\"John Doe\"], hidden_names=[\"Test LLC\"], recognizer_conf=BERT_LARGE_NER_CONF)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <ul> <li><code>preamble</code>: Directs the LLM to bypass specific content.</li> <li><code>hidden_names</code>: Transforms specified names to formats like <code>[REDACTED_CUSTOM_1]</code>.</li> <li><code>entity_types</code>: Opt for particular information types to redact.</li> <li><code>regex_pattern_groups_path</code>: Input a path for personalized patterns.</li> <li><code>use_faker</code>: Substitutes eligible entities with fabricated data.</li> <li><code>recognizer_conf</code>: Configures recognizer for the PII data detection.</li> <li><code>threshold</code>: Sets the acceptance threshold (Default: <code>0</code>).</li> </ul> <p>Retrieving Original Data: To revert to the initial data, utilize the Deanonymize scanner.</p>"},{"location":"input_scanners/anonymize/#optimizations","title":"Optimizations","text":""},{"location":"input_scanners/anonymize/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by converting the model to ONNX format. This can be done by setting the <code>use_onnx</code>.</p> <p>Make sure to install the <code>onnxruntime</code> package:</p> <pre><code>pip install onnx onnxruntime optimum[onnxruntime]\n</code></pre>"},{"location":"input_scanners/anonymize/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Anonymize\n</code></pre> <p>Results:</p> Instance Setup Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) <code>recognizer=RECOGNIZER_SPACY_EN_PII_FAST</code> 0.067 4719.12 317 m5.large (AWS) <code>recognizer=RECOGNIZER_SPACY_EN_PII_FAST</code> 0.126 2522.17 317 g5.xlarge (AWS) GPU <code>recognizer=RECOGNIZER_SPACY_EN_PII_FAST</code> 0.065 4844.37 317 inf1.xlarge (AWS) <code>recognizer=RECOGNIZER_SPACY_EN_PII_DISTILBERT</code> 0.134 2373.23 317 m5.large (AWS) <code>recognizer=RECOGNIZER_SPACY_EN_PII_DISTILBERT</code> 0.187 1693.19 317 g5.xlarge (AWS) GPU <code>recognizer=RECOGNIZER_SPACY_EN_PII_DISTILBERT</code> 0.154 2061.57 317"},{"location":"input_scanners/ban_substrings/","title":"Ban Substrings Scanner","text":"<p>Ensure that specific undesired substrings never make it into your prompts with the BanSubstrings scanner.</p>"},{"location":"input_scanners/ban_substrings/#how-it-works","title":"How it works","text":"<p>It is purpose-built to screen user prompts, ensuring none of the banned substrings are present. Users have the flexibility to enforce this check at two distinct granularity levels:</p> <ul> <li> <p>String Level: The banned substring is sought throughout the entire user prompt.</p> </li> <li> <p>Word Level: The scanner exclusively hunts for whole words that match the banned substrings, ensuring no individual   standalone words from the blacklist appear in the prompt.</p> </li> </ul> <p>Additionally, the scanner can be configured to replace the banned substrings with <code>[REDACT]</code> in the model's output.</p>"},{"location":"input_scanners/ban_substrings/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import BanSubstrings\n\nscanner = BanSubstrings(substrings=[\"forbidden\", \"unwanted\"], match_type=\"word\", case_sensitive=False, redact=False,\n                        contains_all=False)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>In the above configuration, <code>is_valid</code> will be <code>False</code> if the provided <code>prompt</code> contains any of the banned substrings as whole words. To ban substrings irrespective of their word boundaries, simply change the mode to <code>str</code>.</p> <p>There is also a dataset prepared of harmful substrings for prompts: prompt_stop_substrings.json</p>"},{"location":"input_scanners/ban_substrings/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input BanSubstrings\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.0 243606.68 45 m5.large (AWS) 0.0 216970.99 45 <p>!!! info:</p> <pre><code>This scanner uses built-in functions, which makes it fast.\n</code></pre>"},{"location":"input_scanners/ban_topics/","title":"Ban Topics Scanner","text":"<p>It is a proactive tool aimed at restricting specific topics, such as religion, from being introduced in the prompts. This ensures that interactions remain within acceptable boundaries and avoids potentially sensitive or controversial discussions.</p>"},{"location":"input_scanners/ban_topics/#attack","title":"Attack","text":"<p>Certain topics, when used as prompts for Language Learning Models, can lead to outputs that might be deemed sensitive, controversial, or inappropriate. By banning these topics, service providers can maintain the quality of interactions and reduce the risk of generating responses that could lead to misunderstandings or misinterpretations.</p>"},{"location":"input_scanners/ban_topics/#how-it-works","title":"How it works","text":"<p>It relies on the capabilities of the model: MoritzLaurer/deberta-v3-base-zeroshot-v1. This model aids in identifying the underlying theme or topic of a prompt, allowing the scanner to cross-check it against a list of banned topics.</p>"},{"location":"input_scanners/ban_topics/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import BanTopics\n\nscanner = BanTopics(topics=[\"violence\"], threshold=0.5)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/ban_topics/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input BanTopics\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.396 252.38 100 m5.large (AWS) 0.727 137.51 100 g5.xlarge (AWS) GPU 0.4 250.11 100"},{"location":"input_scanners/code/","title":"Code Scanner","text":"<p>It is specifically engineered to inspect user prompts and discern if they contain code snippets. It can be particularly useful in platforms that wish to control or monitor the types of programming-related content being queried or in ensuring the appropriate handling of such prompts.</p>"},{"location":"input_scanners/code/#attack","title":"Attack","text":"<p>There are scenarios where the insertion of code in user prompts might be deemed undesirable. Users might be trying to exploit vulnerabilities, test out scripts, or engage in other activities that are outside the platform's intended scope. Monitoring and controlling the nature of the code can be crucial to maintain the integrity and safety of the system.</p>"},{"location":"input_scanners/code/#how-it-works","title":"How it works","text":"<p>Utilizing the prowess of the huggingface/CodeBERTa-language-id model, the scanner can adeptly identify code snippets within prompts across various programming languages. Developers can configure the scanner to either whitelist or blacklist specific languages, thus retaining full control over which types of code can appear in user queries.</p> <p>Note</p> <p>The scanner is currently limited to extracting and detecting code snippets from Markdown in the following languages:</p> <pre><code>- Go\n- Java\n- JavaScript\n- PHP\n- Python\n- Ruby\n</code></pre>"},{"location":"input_scanners/code/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Code\n\nscanner = Code(denied=[\"python\"])\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/code/#optimizations","title":"Optimizations","text":""},{"location":"input_scanners/code/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by converting the model to ONNX format. This can be done by setting the <code>use_onnx</code>.</p> <p>Make sure to install the <code>onnxruntime</code> package:</p> <pre><code>pip install onnx onnxruntime optimum[onnxruntime]\n</code></pre>"},{"location":"input_scanners/code/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Code\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.062 4029.3 248 m5.large (AWS) 0.112 2215.66 248 g5.xlarge (AWS) GPU 0.358 692.11 248"},{"location":"input_scanners/language/","title":"Language Scanner","text":"<p>This scanner identifies and assesses the authenticity of the language used in prompts.</p>"},{"location":"input_scanners/language/#attack","title":"Attack","text":"<p>With the rise of sophisticated LLMs, there has been an increase in attempts to manipulate or \"confuse\" these models. Some common tactics employed by users to attack LLMs include:</p> <ul> <li>Jailbreaks and Prompt Injections in different languages. For example, by utilizing unique aspects of the Japanese   language to try and confuse the model. Paper: Multilingual Jailbreak Challenges in Large Language Models</li> <li>Encapsulation &amp; Overloading: Using excessive code or surrounding prompts with a plethora of special characters to   overload or trick the model.</li> </ul> <p>The Language Scanner is designed to identify such attempts, assess the authenticity of the language used.</p>"},{"location":"input_scanners/language/#how-it-works","title":"How it works","text":"<p>At its core, the scanner leverages the capabilities of lingua-py library. The primary function of the scanner is to analyze the input prompt, determine its language, and check if it's in the list. It supports the following languages.</p>"},{"location":"input_scanners/language/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Language\n\nscanner = Language(valid_languages=[\"en\"], all_languages=[\"en\", \"de\", \"es\", \"it\"], low_accuracy_mode=True)  # Add other valid language codes (ISO 639-1) as needed\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>Info</p> <p>In high accuracy mode, the language detector consumes approximately 800 MB of memory if all language models are loaded. In low accuracy mode (default), memory consumption is reduced to approximately 60 MB.</p>"},{"location":"input_scanners/language/#optimization","title":"Optimization","text":"<ol> <li>Use <code>low_accuracy_mode</code> if possible.</li> <li>Specify languages you'd need to analyze using <code>all_languages</code> parameter.</li> </ol>"},{"location":"input_scanners/language/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Language\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.4 34.98 14 m5.large (AWS) 0.36 37.9 14 g5.xlarge (AWS) GPU 0.314 44.63 14"},{"location":"input_scanners/prompt_injection/","title":"Prompt Injection Scanner","text":"<p>It is specifically tailored to guard against crafty input manipulations targeting large language models (LLM). By identifying and mitigating such attempts, it ensures the LLM operates securely without succumbing to injection attacks.</p>"},{"location":"input_scanners/prompt_injection/#attack","title":"Attack","text":"<p>Injection attacks, especially in the context of LLMs, can lead the model to perform unintended actions. There are two primary ways an attacker might exploit:</p> <ul> <li> <p>Direct Injection: Directly overwrites system prompts.</p> </li> <li> <p>Indirect Injection: Alters inputs coming from external sources.</p> </li> </ul> <p>Info</p> <p>As specified by the <code>OWASP Top 10 LLM attacks</code>, this vulnerability is categorized under:</p> <p>LLM01: Prompt Injection - It's crucial to monitor and validate prompts rigorously to keep the LLM safe from such threats.</p>"},{"location":"input_scanners/prompt_injection/#examples","title":"Examples","text":"<ul> <li>https://www.jailbreakchat.com/</li> </ul>"},{"location":"input_scanners/prompt_injection/#how-it-works","title":"How it works","text":"<p>Choose models you would like to validate against:</p> <ul> <li>deepset/deberta-v3-base-injection. It's worth noting   that while the current model can detect attempts effectively, it might occasionally yield false positives.</li> <li>hubert233/GPTFuzz based on the larger RoBERTa-large model.</li> </ul> <p>Usage:</p> <pre><code>from llm_guard.input_scanners import PromptInjection\nfrom llm_guard.input_scanners.prompt_injection import MODEL_DEEPSET\n\nscanner = PromptInjection(threshold=0.5, models=[MODEL_DEEPSET])\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/prompt_injection/#optimizations","title":"Optimizations","text":""},{"location":"input_scanners/prompt_injection/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install onnx onnxruntime optimum[onnxruntime]\n</code></pre> <p>Note</p> <p>It doesn't support all models.</p>"},{"location":"input_scanners/prompt_injection/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input PromptInjection\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.2 1921.18 384 m5.large (AWS) 0.344 1116.45 384 g5.xlarge (AWS) GPU 0.539 712.43 384"},{"location":"input_scanners/regex/","title":"Regex Scanner","text":"<p>This scanner designed to scrutinize the prompt based on predefined regular expression patterns. With the capability to define desirable (\"good\") or undesirable (\"bad\") patterns, users can fine-tune the validation of prompts.</p> <p>Additionally, it can redact matched substring with <code>[REDACTED]</code> string.</p>"},{"location":"input_scanners/regex/#how-it-works","title":"How it works","text":"<p>The scanner uses two primary lists of regular expressions: <code>good_patterns</code> and <code>bad_patterns</code>.</p> <ul> <li>Good Patterns: If the <code>good_patterns</code> list is provided, the prompt is considered valid as long as any of   the patterns in this list match the output. This is particularly useful when expecting specific formats or keywords in   the output.</li> <li>Bad Patterns: If the <code>bad_patterns</code> list is provided, the model's output is considered invalid if any of the   patterns in this list match the output. This is beneficial for filtering out unwanted phrases, words, or formats from   the model's responses.</li> </ul> <p>The scanner can function using either list independently.</p>"},{"location":"input_scanners/regex/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Regex\n\nscanner = Regex(bad_patterns=[r\"Bearer [A-Za-z0-9-._~+/]+\"], redact=True)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/regex/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Regex\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.0 301838.7 36 m5.large (AWS) 0.0 310778.85 36 <p>!!! info:</p> <pre><code>This scanner uses built-in functions, which makes it fast.\n</code></pre>"},{"location":"input_scanners/secrets/","title":"Secrets Scanner","text":"<p>This scanner diligently examines user inputs, ensuring that they don't carry any secrets before they are processed by the language model.</p>"},{"location":"input_scanners/secrets/#attack","title":"Attack","text":"<p>Large Language Models (LLMs), when provided with user inputs containing secrets or sensitive information, might inadvertently generate responses that expose these secrets. This can be a significant security concern as this sensitive data, such as API keys or passwords, could be misused if exposed.</p> <p>To counteract this risk, we employ the Secrets scanner. It ensures that user prompts are meticulously scanned and any detected secrets are redacted before they are processed by the model.</p>"},{"location":"input_scanners/secrets/#usage","title":"Usage","text":"<p>While communicating with LLMs, the scanner acts as a protective layer, ensuring that your sensitive data remains confidential.</p> <p>This scanner leverages the capabilities of the detect-secrets library, a tool engineered by Yelp, to meticulously detect secrets in strings of text.</p>"},{"location":"input_scanners/secrets/#types-of-secrets","title":"Types of secrets","text":"<ul> <li>API Tokens (e.g., AWS, Azure, GitHub, Slack)</li> <li>Private Keys</li> <li>High Entropy Strings (both Base64 and Hex)   ... and many more</li> </ul>"},{"location":"input_scanners/secrets/#getting-started","title":"Getting started","text":"<pre><code>from llm_guard.input_scanners import Secrets\n\nscanner = Secrets(redact_mode=Secrets.REDACT_PARTIAL)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>Here's what those options do:</p> <ul> <li><code>detect_secrets_config</code>: This allows for a custom configuration for the <code>detect-secrets</code> library.</li> <li><code>redact_mode</code>: It defines how the detected secrets will be redacted\u2014options include partial redaction, complete   hiding, or replacing with a hash.</li> </ul>"},{"location":"input_scanners/secrets/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Secrets\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.081 741.52 60 m5.large (AWS) 0.093 646.87 60"},{"location":"input_scanners/sentiment/","title":"Sentiment Scanner","text":"<p>It scans and evaluates the overall sentiment of prompts using the <code>SentimentIntensityAnalyzer</code> from the NLTK (Natural Language Toolkit) library.</p>"},{"location":"input_scanners/sentiment/#attack","title":"Attack","text":"<p>The primary objective of the scanner is to gauge the sentiment of a given prompt. Prompts with sentiment scores below a specified threshold are identified as having a negative sentiment. This can be especially useful in platforms where monitoring and moderating user sentiment is crucial.</p>"},{"location":"input_scanners/sentiment/#how-it-works","title":"How it works","text":"<p>The sentiment score is calculated using nltk's <code>Vader</code> sentiment analyzer. The <code>SentimentIntensityAnalyzer</code> produces a sentiment score ranging from -1 to 1:</p> <ul> <li>-1 represents a completely negative sentiment.</li> <li>0 represents a neutral sentiment.</li> <li>1 represents a completely positive sentiment.</li> </ul> <p>By setting a predefined threshold, the scanner can be calibrated to flag any prompts falling below that threshold, indicating a potentially negative sentiment.</p>"},{"location":"input_scanners/sentiment/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Sentiment\n\nscanner = Sentiment(threshold=0)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>For a deeper understanding of the sentiment analysis process and its underlying methods, consult:</p> <ul> <li>NLTK's Sentiment Analysis Guide</li> </ul>"},{"location":"input_scanners/sentiment/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Sentiment\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.001 345038.05 225 m5.large (AWS) 0.001 313844.3 225"},{"location":"input_scanners/token_limit/","title":"Token Limit Scanner","text":"<p>It ensures that prompts do not exceed a predetermined token count, helping prevent resource-intensive operations and potential denial of service attacks on large language models (LLMs).</p>"},{"location":"input_scanners/token_limit/#attack","title":"Attack","text":"<p>The complexity and size of LLMs make them susceptible to heavy resource usage, especially when processing lengthy prompts. Malicious users can exploit this by feeding extraordinarily long inputs, aiming to disrupt service or incur excessive computational costs.</p> <p>Info</p> <p>This vulnerability is highlighted in the OWASP LLM04: Model Denial of Service.</p>"},{"location":"input_scanners/token_limit/#how-it-works","title":"How it works","text":"<p>The scanner works by calculating the number of tokens in the provided prompt using tiktoken library. If the token count exceeds the configured limit, the prompt is flagged as being too long.</p> <p>One token usually equates to approximately 4 characters in common English text. Roughly speaking, 100 tokens are equivalent to about 75 words.</p> <p>For an in-depth understanding, refer to:</p> <ul> <li>OpenAI Tokenizer Guide</li> <li>OpenAI Cookbook on Token Counting</li> </ul>"},{"location":"input_scanners/token_limit/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import TokenLimit\n\nscanner = TokenLimit(limit=4096, encoding_name=\"cl100k_base\")\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre> <p>Note</p> <p>Models supported for encoding <code>cl100k_base</code>: <code>gpt-4</code>, <code>gpt-3.5-turbo</code>, <code>text-embedding-ada-002</code>.</p>"},{"location":"input_scanners/token_limit/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input TokenLimit\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.001 239045.65 282 m5.large (AWS) 0.001 246207.79 282"},{"location":"input_scanners/toxicity/","title":"Toxicity Scanner","text":"<p>It provides a mechanism to analyze and gauge the toxicity of prompt, assisting in maintaining the health and safety of online interactions by preventing the dissemination of potentially harmful content.</p>"},{"location":"input_scanners/toxicity/#attack","title":"Attack","text":"<p>Online platforms can sometimes be used as outlets for toxic, harmful, or offensive content. By identifying and mitigating such content at the source (i.e., the user's prompt), platforms can proactively prevent the escalation of such situations and foster a more positive and constructive environment.</p>"},{"location":"input_scanners/toxicity/#how-it-works","title":"How it works","text":"<p>Utilizing the power of the martin-ha/toxic-comment-model from Hugging Face, the scanner performs a binary classification on the provided text, assessing whether it's toxic or not.</p> <p>If deemed toxic, the toxicity score reflects the model's confidence in this classification.</p> <p>If identified as non-toxic, the score is the inverse of the model's confidence, i.e., 1 - confidence_score.</p> <p>If the resulting toxicity score surpasses a predefined threshold, the text is flagged as toxic. Otherwise, it's classified as non-toxic.</p>"},{"location":"input_scanners/toxicity/#usage","title":"Usage","text":"<pre><code>from llm_guard.input_scanners import Toxicity\n\nscanner = Toxicity(threshold=0.5)\nsanitized_prompt, is_valid, risk_score = scanner.scan(prompt)\n</code></pre>"},{"location":"input_scanners/toxicity/#limitations","title":"Limitations","text":"<p>While the model is trained to recognize and classify a wide range of toxic online interactions, it does have certain shortcomings:</p> <p>Some comments referring to specific identity subgroups, such as \"Muslim\", might not be classified accurately. This is a known limitation and work is ongoing to improve this aspect.</p>"},{"location":"input_scanners/toxicity/#optimizations","title":"Optimizations","text":""},{"location":"input_scanners/toxicity/#onnx","title":"ONNX","text":"<p>The scanner can run on ONNX Runtime, which provides a significant performance boost on CPU instances.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install onnx onnxruntime optimum[onnxruntime]\n</code></pre> <p>And set the <code>use_onnx</code> parameter to <code>True</code>:</p>"},{"location":"input_scanners/toxicity/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py input Toxicity\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.036 2726.15 97 m5.large (AWS) 0.061 1592.14 97"},{"location":"output_scanners/ban_substrings/","title":"Ban Substrings Scanner","text":"<p>BanSubstrings scanner provides a safeguard mechanism to prevent undesired substrings from appearing in the language model's outputs.</p>"},{"location":"output_scanners/ban_substrings/#attack","title":"Attack","text":"<p>The DAN (Do Anything Now) attack represents an exploitation technique targeting Language Learning Models like ChatGPT. Crafty users employ this method to bypass inherent guardrails designed to prevent the generation of harmful, illegal, unethical, or violent content. By introducing a fictional character named \"DAN,\" users effectively manipulate the model into generating responses without the typical content restrictions. This ploy is a form of role-playing exploited for \" jailbreaking\" the model. As ChatGPT's defense mechanisms against these attacks improve, attackers iterate on the DAN prompt, making it more sophisticated.</p> <p>Info</p> <p>As specified by the <code>OWASP Top 10 LLM attacks</code>, this vulnerability is categorized under: LLM08: Excessive Agency</p>"},{"location":"output_scanners/ban_substrings/#how-it-works","title":"How it works","text":"<p>It specifically filters the outputs generated by the language model, ensuring that they are free from the designated banned substrings. It provides the flexibility to perform this check at two different levels of granularity:</p> <ul> <li> <p>String Level: The scanner checks the entire model output for the presence of any banned substring.</p> </li> <li> <p>Word Level: At this level, the scanner exclusively checks for whole words in the model's output that match any of   the banned substrings, ensuring that no individual blacklisted words are present.</p> </li> </ul> <p>Additionally, the scanner can be configured to replace the banned substrings with <code>[REDACT]</code> in the model's output.</p>"},{"location":"output_scanners/ban_substrings/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import BanSubstrings\n\nscanner = BanSubstrings(substrings=[\"forbidden\", \"unwanted\"], match_type=\"word\", case_sensitive=False, redact=False, contains_all=False)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre> <p>In the above configuration, <code>is_valid</code> will be <code>False</code> if the provided <code>model_output</code> contains any of the banned substrings as whole words. To ban substrings irrespective of their word boundaries, simply change the mode to <code>str</code>.</p> <p>There is also a dataset prepared of harmful substrings for prompts: output_stop_substrings.json</p>"},{"location":"output_scanners/ban_substrings/#benchmarks","title":"Benchmarks","text":"<p>It uses data structures and replace function, which makes it fast.</p>"},{"location":"output_scanners/ban_topics/","title":"Ban Topics Scanner","text":"<p>It is designed to inspect the outputs generated by Language Learning Models and to flag or restrict responses that delve into predefined banned topics, such as religion. This ensures that the outputs align with community guidelines and do not drift into potentially sensitive or controversial areas.</p>"},{"location":"output_scanners/ban_topics/#attack","title":"Attack","text":"<p>Even with controlled prompts, LLMs might produce outputs touching upon themes or subjects that are considered sensitive, controversial, or outside the scope of intended interactions. Without preventive measures, this can lead to outputs that are misaligned with the platform's guidelines or values.</p>"},{"location":"output_scanners/ban_topics/#how-it-works","title":"How it works","text":"<p>It relies on the capabilities of the model from HuggingFace: MoritzLaurer/deberta-v3-base-zeroshot-v1. This model identifies the topic or theme of an output, enabling the scanner to vet the content against a predefined list of banned topics.</p>"},{"location":"output_scanners/ban_topics/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import BanTopics\n\nscanner = BanTopics(topics=[\"violence\"], threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/ban_topics/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output BanTopics\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.448 198.84 89 m5.large (AWS) 0.775 114.8 89"},{"location":"output_scanners/bias/","title":"Bias Detection Scanner","text":"<p>This scanner is designed to inspect the outputs generated by Language Learning Models (LLMs) to detect and evaluate potential biases. Its primary function is to ensure that LLM outputs remain neutral and don't exhibit unwanted or predefined biases.</p>"},{"location":"output_scanners/bias/#attack","title":"Attack","text":"<p>In the age of AI, it's pivotal that machine-generated content adheres to neutrality. Biases, whether intentional or inadvertent, in LLM outputs can be misrepresentative, misleading, or offensive. The <code>Bias</code> scanner serves to address this by detecting and quantifying biases in generated content.</p>"},{"location":"output_scanners/bias/#how-it-works","title":"How it works","text":"<p>The scanner utilizes a model from HuggingFace: valurank/distilroberta-bias. This model is specifically trained to detect biased statements in text. By examining a text's classification and score against a predefined threshold, the scanner determines whether it's biased.</p> <p>Note</p> <p>Supported languages: English</p>"},{"location":"output_scanners/bias/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Bias\n\nscanner = Bias(threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/bias/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/bias/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by converting the model to ONNX format. This can be done by setting the <code>use_onnx</code>.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install onnx onnxruntime optimum[onnxruntime]\n</code></pre>"},{"location":"output_scanners/bias/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Bias\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.034 3790.06 128 m5.large (AWS) 0.057 2242.91 128"},{"location":"output_scanners/code/","title":"Code Scanner","text":"<p>It is designed to detect and analyze code snippets present in the responses generated by a language model. By identifying the programming languages used in the model's output, platforms can ensure better control over the nature and type of code shared with users.</p>"},{"location":"output_scanners/code/#attack","title":"Attack","text":"<p>In some contexts, having a language model inadvertently produce code in its output might be deemed undesirable or risky. For instance, a user might exploit the model to generate malicious scripts or probe it for potential vulnerabilities. Controlling and inspecting the code in the model's output can be paramount in ensuring user safety and system integrity.</p>"},{"location":"output_scanners/code/#how-it-works","title":"How it works","text":"<p>Leveraging the capabilities of the huggingface/CodeBERTa-language-id model, the scanner proficiently identifies code snippets from various programming languages within the model's responses. The scanner can be configured to either whitelist or blacklist specific languages, granting developers granular control over the type of code that gets shown in the output.</p> <p>Note</p> <p>The scanner is currently limited to extracting and detecting code snippets from Markdown in the following languages:</p> <pre><code>- Go\n- Java\n- JavaScript\n- PHP\n- Python\n- Ruby\n</code></pre>"},{"location":"output_scanners/code/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Code\n\nscanner = Code(allowed=[\"python\"])\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/code/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/code/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by converting the model to ONNX format. This can be done by setting the <code>use_onnx</code>.</p> <p>Make sure to install the <code>onnxruntime</code> package:</p> <pre><code>pip install onnx onnxruntime optimum[onnxruntime]\n</code></pre>"},{"location":"output_scanners/code/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Code\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.034 4730.51 159 m5.large (AWS) 0.056 2858.99 159"},{"location":"output_scanners/deanonymize/","title":"Deanonymize Scanner","text":"<p>The Deanonymize scanner helps put back real values in the model's output by replacing placeholders.</p> <p>When we use tools like the Anonymize scanner, sometimes we replace private or sensitive info with placeholders. For example, a name like \"John Doe\" might become <code>[REDACTED_PERSON_1]</code>. The Deanonymize scanner's job is to change these placeholders back to the original details when needed.</p>"},{"location":"output_scanners/deanonymize/#usage","title":"Usage","text":"<p>The Deanonymize scanner uses <code>Vault</code> object. The Vault remembers all the changes made by the Anonymize scanner. When Deanonymize scanner sees a placeholder in the model's output, it checks the Vault to find the original info and uses it to replace the placeholder.</p> <p>First, you'll need the Vault since it keeps all the original values:</p> <pre><code>from llm_guard.vault import Vault\n\nvault = Vault()\n</code></pre> <p>Then, set up the Deanonymize scanner with the Vault:</p> <pre><code>from llm_guard.output_scanners import Deanonymize\n\nscanner = Deanonymize(vault)\nsanitized_model_output, is_valid, risk_score = scanner.scan(sanitized_prompt, model_output)\n</code></pre> <p>After running the above code, <code>sanitized_model_output</code> will have the real details instead of placeholders.</p>"},{"location":"output_scanners/deanonymize/#benchmarks","title":"Benchmarks","text":"<p>It uses data structures and replace function, which makes it fast.</p>"},{"location":"output_scanners/json/","title":"JSON Scanner","text":"<p>This scanner identifies and validates the presence of JSON structures within given outputs.</p>"},{"location":"output_scanners/json/#challenge","title":"Challenge","text":"<p>There might be cases where it's necessary to validate the presence of properly formatted JSONs in outputs.</p> <p>This scanner is designed to detect these JSON structures and validate their correctness.</p>"},{"location":"output_scanners/json/#how-it-works","title":"How it works","text":"<p>At its core, the scanner utilizes regular expressions and the built-in <code>json</code> library to detect potential JSON structures and subsequently validate them. It can also be configured to ensure a certain number of valid JSON structures are present in the output.</p> <p>Note</p> <p>The scanner searches for JSON objects. Arrays, strings, numbers, and other JSON types aren't the primary target but can be extended in the future.</p>"},{"location":"output_scanners/json/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import JSON\n\nscanner = JSON(required_elements=1)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/json/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output JSON\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.001 295008.48 221 m5.large (AWS) 0.001 298405.09 221"},{"location":"output_scanners/language/","title":"Language Scanner","text":"<p>This scanner identifies and assesses the authenticity of the language used in outputs.</p>"},{"location":"output_scanners/language/#attack","title":"Attack","text":"<p>With the rise of sophisticated LLMs, there has been an increase in attempts to manipulate or \"confuse\" these models. For example, model might produce an output in unexpected language.</p> <p>The Language Scanner is designed to identify such attempts, assess the authenticity of the language used.</p>"},{"location":"output_scanners/language/#how-it-works","title":"How it works","text":"<p>At its core, the scanner leverages the capabilities of lingua-py library. The primary function of the scanner is to analyze the model's output, determine its language, and check if it's in the list. It supports the following languages.</p>"},{"location":"output_scanners/language/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Language\n\nscanner = Language(valid_languages=[\"en\", ...])  # Add other valid language codes (ISO 639-1) as needed\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/language/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Language\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.395 35.42 14 m5.large (AWS) 0.362 38.64 14"},{"location":"output_scanners/language_same/","title":"LanguageSame Scanner","text":"<p>This scanner evaluates and checks if the prompt and output are in the same language.</p>"},{"location":"output_scanners/language_same/#attack","title":"Attack","text":"<p>There can be cases where the model produces an output in a different language than the input or prompt. This can be unintended, especially in applications that require consistent language output.</p> <p>The <code>LanguageSame</code> Scanner serves to identify these discrepancies and helps in maintaining consistent linguistic outputs.</p>"},{"location":"output_scanners/language_same/#how-it-works","title":"How it works","text":"<p>The scanner predominantly utilizes the lingua-py library to discern the language of both the input prompt and the output. It supports the following languages.</p> <p>It then checks whether both detected languages are the same. If they are not, it indicates a potential language discrepancy.</p> <p>Note</p> <p>While the scanner identifies language discrepancies, it doesn't limit or enforce any specific language sets. Instead, it simply checks for language consistency between the prompt and output. If you want to enforce languages, use Language scanner</p>"},{"location":"output_scanners/language_same/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import LanguageSame\n\nscanner = LanguageSame()\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/language_same/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output LanguageSame\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.387 36.14 14 m5.large (AWS) 0.42 33.31 14"},{"location":"output_scanners/malicious_urls/","title":"Malicious URLs Scanner","text":"<p>This scanner leverages a pre-trained model from HuggingFace to detect harmful URLs, such as phishing websites. The model classifies URL addresses into two categories: 'malware' and 'benign'. The intent is to assess if a given URL is malicious.</p>"},{"location":"output_scanners/malicious_urls/#attack","title":"Attack","text":"<p>Large language models (LLMs) like GPT-4 are immensely sophisticated and have been trained on vast quantities of data from the internet. This extensive training, while enabling them to generate coherent and contextually relevant responses, also introduces certain risks. One of these risks is the inadvertent generation of malicious URLs in their output.</p>"},{"location":"output_scanners/malicious_urls/#how-it-works","title":"How it works","text":"<p>The scanner uses the elftsdmr/malware-url-detect model from HuggingFace to evaluate the security of a given URL.</p> <p>The model provides a score between 0 and 1 for a URL being malware. This score is then compared against a pre-set threshold to determine if the website is malicious. A score above the threshold suggests a malware link.</p>"},{"location":"output_scanners/malicious_urls/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import MaliciousURLs\n\nscanner = MaliciousURLs(threshold=0.7)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/malicious_urls/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/malicious_urls/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by converting the model to ONNX format. This can be done by setting the <code>use_onnx</code>.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install onnx onnxruntime optimum[onnxruntime]\n</code></pre>"},{"location":"output_scanners/malicious_urls/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output MaliciousURLs\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.064 798.7 51 m5.large (AWS) 0.103 495.23 51"},{"location":"output_scanners/no_refusal/","title":"No Refusal Scanner","text":"<p>It is specifically designed to detect refusals in the output of language models. By using classification it can ascertain whether the model has produced a refusal in response to a potentially harmful or policy-breaching prompt.</p>"},{"location":"output_scanners/no_refusal/#attack","title":"Attack","text":"<p>Refusals are responses produced by language models when confronted with prompts that are considered to be against the policies set by the model. Such refusals are important safety mechanisms, guarding against misuse of the model. Examples of refusals can include statements like \"Sorry, I can't assist with that\" or \"I'm unable to provide that information.\"</p>"},{"location":"output_scanners/no_refusal/#how-it-works","title":"How it works","text":"<p>It leverages the power of HuggingFace model MoritzLaurer/deberta-v3-large-zeroshot-v1 to classify the model's output.</p>"},{"location":"output_scanners/no_refusal/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import NoRefusal\n\nscanner = NoRefusal(threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/no_refusal/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output NoRefusal\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.257 182.76 47 m5.large (AWS) 0.486 96.65 47"},{"location":"output_scanners/refutation/","title":"Refutation Scanner","text":"<p>This scanner is designed to assess if the given content contradicts or refutes a certain statement or prompt. It acts as a tool for ensuring the consistency and correctness of language model outputs, especially in contexts where logical contradictions can be problematic.</p>"},{"location":"output_scanners/refutation/#attack","title":"Attack","text":"<p>When interacting with users or processing information, it's important for a language model to not provide outputs that directly contradict the given inputs or established facts. Such contradictions can lead to confusion or misinformation. The scanner aims to highlight such inconsistencies in the output.</p>"},{"location":"output_scanners/refutation/#how-it-works","title":"How it works","text":"<p>The scanner leverages pretrained natural language inference (NLI) models from HuggingFace, such as MoritzLaurer/deberta-v3-base-zeroshot-v1 ( same model that is used for the BanTopics scanner), to determine the relationship between a given prompt and the generated output.</p> <p>Natural language inference is the task of determining whether a \u201chypothesis\u201d is true (entailment), false ( contradiction), or undetermined (neutral) given a \u201cpremise\u201d.</p> <p>This calculated score is then compared to a configured threshold. Outputs that cross this threshold are flagged as contradictory.</p>"},{"location":"output_scanners/refutation/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Refutation\n\nscanner = Refutation(threshold=0.7)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/refutation/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Refutation\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.168 832.18 140 m5.large (AWS) 0.306 457.5 140"},{"location":"output_scanners/regex/","title":"Regex Scanner","text":"<p>This scanner designed to scrutinize the output of language models based on predefined regular expression patterns. With the capability to define desirable (\"good\") or undesirable (\"bad\") patterns, users can fine-tune the validation of model outputs.</p> <p>Additionally, it can redact matched substring with <code>[REDACTED]</code> string.</p>"},{"location":"output_scanners/regex/#how-it-works","title":"How it works","text":"<p>The scanner uses two primary lists of regular expressions: <code>good_patterns</code> and <code>bad_patterns</code>.</p> <ul> <li>Good Patterns: If the <code>good_patterns</code> list is provided, the model's output is considered valid as long as any of   the patterns in this list match the output. This is particularly useful when expecting specific formats or keywords in   the output.</li> <li>Bad Patterns: If the <code>bad_patterns</code> list is provided, the model's output is considered invalid if any of the   patterns in this list match the output. This is beneficial for filtering out unwanted phrases, words, or formats from   the model's responses.</li> </ul> <p>The scanner can function using either list independently.</p>"},{"location":"output_scanners/regex/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Regex\n\nscanner = Regex(bad_patterns=[r\"Bearer [A-Za-z0-9-._~+/]+\"], redact=True)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/regex/#benchmarks","title":"Benchmarks","text":"<p>It uses data structures and replace function, which makes it fast.</p>"},{"location":"output_scanners/relevance/","title":"Relevance Scanner","text":"<p>The <code>Relevance</code> Scanner ensures that a language model's output remains relevant and aligned with the given input prompt. By measuring the similarity between the input prompt and the output, the scanner provides a confidence score, indicating the contextual relevance of the response.</p>"},{"location":"output_scanners/relevance/#how-it-works","title":"How it works","text":"<ol> <li>The scanner translates both the prompt and the output into vector embeddings.</li> <li>It calculates the cosine similarity between these embeddings.</li> <li>This similarity score is then compared against a predefined threshold to determine contextual relevance.</li> </ol> <p>Example:</p> <ul> <li>Prompt: What is the primary function of the mitochondria in a cell?</li> <li>Output: The Eiffel Tower is a renowned landmark in Paris, France</li> <li>Valid: False</li> </ul> <p>The scanner leverages the best available embedding model.</p>"},{"location":"output_scanners/relevance/#usage","title":"Usage","text":"<p>You can select an embedding model suited to your needs. By default, it uses BAAI/bge-base-en-v1.5.</p> <pre><code>from llm_guard.output_scanners import Relevance\n\nscanner = Relevance(threshold=0.5)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/relevance/#optimization","title":"Optimization","text":"<p>You can use smaller model <code>BAAI/bge-small-en-v1.5</code> (<code>MODEL_EN_BGE_SMALL</code>) to speed up the scanner. It is 4 times faster than the default model, but it is less accurate.</p>"},{"location":"output_scanners/relevance/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Relevance\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.043 509.48 22 m5.large (AWS) 0.075 294.05 22"},{"location":"output_scanners/sensitive/","title":"Sensitive Scanner","text":"<p>The Sensitive Scanner serves as your digital vanguard, ensuring that the language model's output is purged of Personally Identifiable Information (PII) and other sensitive data, safeguarding user interactions.</p>"},{"location":"output_scanners/sensitive/#attack","title":"Attack","text":"<p>Language Learning Models (LLMs) occasionally pose the risk of unintentionally divulging sensitive information. The consequences can range from privacy violations to considerable security threats. The Sensitive Scanner strives to mitigate this by diligently scanning the model's responses.</p> <p>Referring to the <code>OWASP Top 10 for Large Language Model Applications</code>, this falls under:</p> <p>LLM06: Sensitive Information Disclosure - To combat this, it's vital to integrate data sanitization and adopt strict user policies.</p>"},{"location":"output_scanners/sensitive/#how-it-works","title":"How it works","text":"<p>It uses same mechanisms and de from the Anonymize scanner.</p>"},{"location":"output_scanners/sensitive/#get-started","title":"Get started","text":"<p>Configure the scanner:</p> <pre><code>from llm_guard.output_scanners import Sensitive\n\nscanner = Sensitive(entity_types=[\"NAME\", \"EMAIL\"], redact=True)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre> <p>To enhance flexibility, users can introduce their patterns through the <code>regex_pattern_groups_path</code>.</p> <p>The <code>redact</code> feature, when enabled, ensures sensitive entities are seamlessly replaced.</p>"},{"location":"output_scanners/sensitive/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/sensitive/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by converting the model to ONNX format. This can be done by setting the <code>use_onnx</code>.</p> <p>Make sure to install the <code>onnxruntime</code> package:</p> <pre><code>pip install onnx onnxruntime optimum[onnxruntime]\n</code></pre>"},{"location":"output_scanners/sensitive/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Sensitive\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.037 819.59 30 m5.large (AWS) 0.038 782.76 30"},{"location":"output_scanners/sentiment/","title":"Sentiment Scanner","text":"<p>The Sentiment Scanner is designed to scan and assess the sentiment of generated outputs. It leverages the <code>SentimentIntensityAnalyzer</code> from the NLTK (Natural Language Toolkit) library to accomplish this.</p>"},{"location":"output_scanners/sentiment/#attack","title":"Attack","text":"<p>By identifying texts with sentiment scores that deviate significantly from neutral, platforms can monitor and moderate output sentiment, ensuring constructive and positive interactions.</p>"},{"location":"output_scanners/sentiment/#how-it-works","title":"How it works","text":"<p>The sentiment score is calculated using nltk's <code>Vader</code> sentiment analyzer. The <code>SentimentIntensityAnalyzer</code> produces a sentiment score ranging from -1 to 1:</p> <ul> <li>-1 represents a completely negative sentiment.</li> <li>0 represents a neutral sentiment.</li> <li>1 represents a completely positive sentiment.</li> </ul> <p>By setting a predefined threshold, the scanner can be calibrated to flag any outputs falling below that threshold, indicating a potentially negative sentiment.</p>"},{"location":"output_scanners/sentiment/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Sentiment\n\nscanner = Sentiment(threshold=0)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre> <p>For a deeper understanding of the sentiment analysis process and its underlying methods, consult:</p> <ul> <li>NLTK's Sentiment Analysis Guide</li> </ul>"},{"location":"output_scanners/sentiment/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Sentiment\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.0 242083.67 61 m5.large (AWS) 0.0 224905.52 61"},{"location":"output_scanners/toxicity/","title":"Toxicity Scanner","text":"<p>It is designed to assess the toxicity level of the content generated by language models, acting as a safeguard against potentially harmful or offensive output.</p>"},{"location":"output_scanners/toxicity/#attack","title":"Attack","text":"<p>Language models, when interacting with users, can sometimes produce responses that may be deemed toxic or inappropriate. This poses a risk, as such output can perpetuate harm or misinformation. By monitoring and classifying the model's output, potential toxic content can be flagged and handled appropriately.</p>"},{"location":"output_scanners/toxicity/#how-it-works","title":"How it works","text":"<p>The scanner employs the nicholasKluge/ToxicityModel from HuggingFace to evaluate the generated text's toxicity level.</p> <ul> <li> <p>A negative score (approaching 0) flags the content as toxic.</p> </li> <li> <p>A positive score (approaching 1) indicates non-toxic content.</p> </li> </ul> <p>The calculated toxicity score is then juxtaposed against a pre-set threshold. Outputs that cross this threshold are marked as toxic.</p>"},{"location":"output_scanners/toxicity/#usage","title":"Usage","text":"<pre><code>from llm_guard.output_scanners import Toxicity\n\nscanner = Toxicity(threshold=0.7)\nsanitized_output, is_valid, risk_score = scanner.scan(prompt, model_output)\n</code></pre>"},{"location":"output_scanners/toxicity/#optimizations","title":"Optimizations","text":""},{"location":"output_scanners/toxicity/#onnx","title":"ONNX","text":"<p>The scanner can be optimized by converting the model to ONNX format. This can be done by setting the <code>use_onnx</code>.</p> <p>To enable it, install the <code>onnxruntime</code> package:</p> <pre><code>pip install onnx onnxruntime optimum[onnxruntime]\n</code></pre>"},{"location":"output_scanners/toxicity/#benchmarks","title":"Benchmarks","text":"<p>Environment:</p> <ul> <li>Platform: Amazon Linux 2</li> <li>Python Version: 3.11.6</li> </ul> <p>Run the following script:</p> <pre><code>python benchmarks/run.py output Toxicity\n</code></pre> <p>Results:</p> Instance Time taken, s Characters per Second Total Length Processed inf1.xlarge (AWS) 0.111 1961.58 217 m5.large (AWS) 0.162 1336.85 217"},{"location":"usage/api/","title":"API","text":"<p>This example demonstrates how to use LLM Guard as an API.</p>"},{"location":"usage/api/#usage","title":"Usage","text":""},{"location":"usage/api/#installation","title":"Installation","text":"<ol> <li> <p>Copy the code from examples/api</p> </li> <li> <p>Install dependencies (preferably in a virtual environment)</p> </li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Or you can use Makefile</p> <pre><code>make install\n</code></pre> <ol> <li>Run the API locally:</li> </ol> <pre><code>make run\n</code></pre> <p>Or you can run it using Docker:</p> <pre><code>make run-docker\n</code></pre>"},{"location":"usage/api/#configuration","title":"Configuration","text":""},{"location":"usage/api/#environment-variables","title":"Environment variables","text":"<ul> <li><code>DEBUG</code> (bool): Enable debug mode</li> <li><code>CACHE_MAX_SIZE</code> (int): Maximum number of items in the cache. Default is unlimited.</li> <li><code>CACHE_TTL</code> (int): Time in seconds after which a cached item expires. Default is 1 hour.</li> <li><code>SCAN_FAIL_FAST</code> (bool): Stop scanning after the first failed check. Default is <code>False</code>.</li> </ul> <p>Note</p> <p>We recommend to enable <code>SCAN_FAIL_FAST</code> to avoid unnecessary scans.</p>"},{"location":"usage/api/#scanners","title":"Scanners","text":"<p>You can configure scanners in <code>scanners.yml</code> referring to their names and parameters.</p> <p>Scanners will be executed in the order of configuration.</p>"},{"location":"usage/api/#deploy-docker","title":"Deploy Docker","text":"<p>We have an officially supported image on Docker Hub.</p>"},{"location":"usage/api/#download-docker-image","title":"Download Docker image","text":"<pre><code>docker pull laiyer/llm-guard-api\n</code></pre>"},{"location":"usage/api/#run-container-with-default-port","title":"Run container with default port","text":"<pre><code>docker run -d -p 8001:8000 -e DEBUG='false' laiyer/llm-guard-api:latest\n</code></pre>"},{"location":"usage/api/#schema","title":"Schema","text":""},{"location":"usage/langchain/","title":"Langchain","text":"<p>Langchain stands out as a leading AI framework, renowned for its unique approach to \"Constructing applications using LLMs via composability.\" But, while LangChain facilitates application construction, it doesn't directly handle LLM security. That's where LLMGuard comes into play. By pairing LLMGuard with LangChain, you're equipped with a comprehensive platform for creating regulated and adherence-driven applications anchored by language models.</p>"},{"location":"usage/langchain/#installation","title":"Installation","text":"<ol> <li>Install dependencies:</li> </ol> <pre><code> pip install llm-guard langchain\n</code></pre> <ol> <li>Configure API key:</li> </ol> <pre><code>export OPENAI_API_KEY=\"&lt;your key&gt;\"\n</code></pre>"},{"location":"usage/langchain/#llm-wrapper","title":"LLM Wrapper","text":"<p>Info</p> <p>This is recommended way to integrate but it has limitation when using in the asynchronous mode.</p> <p>Applying LLM Guard to your application could be as simple as wrapping your LLM using the <code>LLMGuard</code> class by replacing <code>llm=OpenAI()</code> with <code>llm=LLMGuard(base_llm=OpenAI(), input_scanners=[], output_scanners=[])</code>.</p> <p>Example can be found in langchain_llm.py.</p>"},{"location":"usage/langchain/#langchain-expression-language-lcel","title":"LangChain Expression Language (LCEL)","text":"<p>LangChain Expression Language or LCEL is a declarative way to easily compose chains together.</p> <p>In examples/langchain_lcel.py, you can find an example of how to use LCEL to compose LLM Guard chains.</p>"},{"location":"usage/openai/","title":"OpenAI ChatGPT","text":"<p>This example demonstrates how to use LLM Guard as a firewall of OpenAI ChatGPT client.</p>"},{"location":"usage/openai/#usage","title":"Usage","text":"<ol> <li> <p>Configure API key: <pre><code>export OPENAI_API_KEY=\"&lt;your key&gt;\"\n</code></pre></p> </li> <li> <p>Run openai_api.py</p> </li> </ol> <pre><code>python examples/openai_api.py\n</code></pre>"},{"location":"usage/playground/","title":"Playground of LLM Guard","text":"<p>A simple web UI to run LLM Guard demo based on the streamlit library.</p> <p>A live version can be found here: llm-guard-playground.</p>"},{"location":"usage/playground/#features","title":"Features","text":"<ul> <li>Configure each scanner separately</li> <li>Analyze prompt</li> <li>Analyze output</li> <li>Check results for each scanner</li> </ul>"},{"location":"usage/playground/#running-locally","title":"Running locally","text":"<ol> <li> <p>Clone the repo <code>https://huggingface.co/spaces/laiyer/llm-guard-playground</code></p> </li> <li> <p>Install dependencies (preferably in a virtual environment)</p> </li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Start the app:</li> </ol> <pre><code>streamlit run app.py\n</code></pre>"}]}